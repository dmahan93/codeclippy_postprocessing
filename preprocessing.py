# Original file from https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/preprocessing.py
# butchered to directly support what I care about

import gzip
import hashlib
import pickle
import pandas as pd
import pyarrow.parquet as pq
import pyarrow as pa
import copy
import orjson as json
import multiprocessing
import os
import re
import shutil
import time
from multiprocessing import Pool
from pathlib import Path
import boto3
import botocore.exceptions
import xxhash
import numpy as np
import tqdm
from datasets import load_dataset
import os
from concurrent import futures
from arguments import PreprocessingArguments
from transformers import AutoTokenizer, HfArgumentParser
import shutil
from s3transfer.manager import TransferManager
from botocore.session import get_session

PATTERN = re.compile(r"\s+")


def get_hash(example):
    """Get hash of content field."""
    # return {"hash": hashlib.md5(re.sub(PATTERN, "", example["content"]).encode("utf-8")).hexdigest()}
    # return {"hash": hashlib.md5(example["content"].encode("utf-8")).hexdigest()}
    return {"hash": xxhash.xxh128(example["content"].encode("utf-8")).hexdigest()}


def line_stats(example):
    """Calculates mean and max line length of file."""
    line_lengths = [len(line) for line in example["content"].splitlines()]
    return {"line_mean": np.mean(line_lengths), "line_max": max(line_lengths)}


def alpha_stats(example):
    """Calculates mean and max line length of file."""
    # alpha_frac = np.mean([c.isalnum() for c in example["content"]])
    # return {"alpha_frac": alpha_frac}
    return {"alpha_frac": 0.1}


def check_uniques(example, uniques, cached_uniques):
    """Check if current hash is still in set of unique hashes and remove if true."""
    if example["hash"] in cached_uniques:
        return False
    elif example["hash"] in uniques:
        uniques.remove(example["hash"])
        return True
    else:
        return False


def is_autogenerated(example, scan_width=5):
    """Check if file is autogenerated by looking for keywords in the first few lines of the file."""
    keywords = ["auto-generated", "autogenerated", "automatically generated"]
    lines = example["content"].splitlines()
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return {"autogenerated": True}
    else:
        return {"autogenerated": False}


def is_config_or_test(example, scan_width=5, coeff=0.05):
    """Check if file is a configuration file or a unit test by :
    1- looking for keywords in the first few lines of the file.
    2- counting number of occurence of the words 'config' and 'test' with respect to number of lines.
    """

    # keywords = ["unit tests", "test file", "configuration file"]
    # lines = example["content"].splitlines()
    # count_config = 0
    # count_test = 0
    # # first test
    # for _, line in zip(range(scan_width), lines):
    #     for keyword in keywords:
    #         if keyword in line.lower():
    #             return {"config_or_test": True}
    # # second test
    # nlines = example["content"].count("\n")
    # threshold = int(coeff * nlines)
    # for line in lines:
    #     count_config += line.lower().count("config")
    #     count_test += line.lower().count("test")
    #     if count_config > threshold or count_test > threshold:
    #         return {"config_or_test": True}
    return {"config_or_test": False}


def has_no_keywords(example):
    """Check if a python file has none of the keywords for: funcion, class, for loop, while loop."""
    keywords = ["def ", "class ", "for ", "while "]
    lines = example["content"].splitlines()
    for line in lines:
        for keyword in keywords:
            if keyword in line.lower():
                return {"has_no_keywords": False}
    return {"has_no_keywords": True}


def has_few_assignments(example, minimum=4):
    """Check if file uses symbol '=' less than `minimum` times."""
    lines = example["content"].splitlines()
    counter = 0
    for line in lines:
        counter += line.lower().count("=")
        if counter > minimum:
            return {"has_few_assignments": False}
    return {"has_few_assignments": True}


def char_token_ratio(example):
    """Compute character/token ratio of the file with tokenizer."""
    # input_ids = tokenizer(example["content"], truncation=False)["input_ids"]
    # ratio = len(example["content"]) / len(input_ids)
    return {"ratio": 5}


def preprocess(example):
    """Chain all preprocessing steps into one function to not fill cache."""
    results = dict()
    results.update(get_hash(example))
    # results.update(line_stats(example))
    # results.update(alpha_stats(example))
    # results.update(char_token_ratio(example))
    results.update(is_autogenerated(example))
    results.update(is_config_or_test(example))
    # results.update(has_no_keywords(example))
    # results.update(has_few_assignments(example))
    return results


def dsfilter(example, uniques, args, cached_uniques)-> bool:
    """Filter dataset with heuristics. Config, test and has_no_keywords files are removed with a given probability."""
    if not check_uniques(example, uniques, cached_uniques):
        return False
    # elif example["autogenerated"]:
    #     return False
    # elif example["line_max"] > args.line_max:
    #     return False
    # elif example["line_mean"] > args.line_mean:
    #     return False
    # elif example["alpha_frac"] < args.alpha_frac:
    #     return False
    # elif example["ratio"] < args.min_token_ratio:
    #     return False
    # elif example["config_or_test"] and np.random.rand() <= args.filter_proba:
    #     return False
    # elif example["has_no_keywords"] and np.random.rand() <= args.filter_proba:
    #     return False
    # elif example["has_few_assignments"]:
    #     return False
    else:
        return True


def compress_file(file_path):
    """Compress a file with g-zip."""
    with open(file_path, "rb") as f_in:
        with gzip.open(str(file_path) + ".gz", "wb", compresslevel=6) as f_out:
            shutil.copyfileobj(f_in, f_out)
    os.unlink(file_path)


# adapted from https://stackoverflow.com/a/54014862
def bucketkeys(bucket_name, s3_paginator, prefix='/', delimiter='/', start_after=''):
    prefix = prefix[1:] if prefix.startswith(delimiter) else prefix
    start_after = (start_after or prefix) if prefix.endswith(delimiter) else start_after
    for page in s3_paginator.paginate(Bucket=bucket_name, Prefix=prefix, StartAfter=start_after):
        for content in page.get('Contents', ()):
            yield content['Key']


parser = HfArgumentParser(PreprocessingArguments)
args = parser.parse_args()
if args.num_workers is None:
    args.num_workers = multiprocessing.cpu_count()
# tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)


def fetch(key, s3):
    file = f'tmp/{key.split("/")[-1]}'
    with TransferManager(s3) as manager:
        manager.download(bucket_name, key, file)
    # s3.download_file(bucket_name, key, file)
    return file


def fetch_all(keys, s3):

    with futures.ThreadPoolExecutor(max_workers=100) as executor:
        future_to_key = {executor.submit(fetch, key, s3): key for key in keys}

        print("All URLs submitted.")

        for future in futures.as_completed(future_to_key):

            key = future_to_key[future]
            exception = future.exception()

            if not exception:
                yield key, future.result()
            else:
                yield key, exception


def upload(key, s3):
    file = f'outtmp/{key.split("/")[-1]}'
    s3.upload_file(file, bucket_name, key)
    return True


def upload_all(keys, s3):

    with futures.ThreadPoolExecutor(max_workers=100) as executor:
        future_to_key = {executor.submit(upload, key, s3): key for key in keys}

        print("All URLs submitted.")

        for future in futures.as_completed(future_to_key):

            key = future_to_key[future]
            exception = future.exception()

            if not exception:
                yield key, future.result()
            else:
                yield key, exception


def get_json(filename):
    with gzip.open(filename, 'rb') as f:
        data = f.read().decode(encoding='utf-8')
        data = data.split('\n')
        json_data = [json.loads(data_item) for data_item in data if data_item != '']
    return json_data


def write_json(in_data):
    filename, data = in_data
    with gzip.open(filename, 'w') as fout:
        fout.write(data)


if __name__ == '__main__':
    # Settings

    if not os.path.isdir('tmp'):
        os.mkdir('tmp')
    if not os.path.isdir('outtmp'):
        os.mkdir('outtmp')
    cached_uniques = set()

    s3 = boto3.client('s3')
    bucket_name = args.bucket_name
    with Pool(args.num_workers) as mypool:
        try:
            keys = list(bucketkeys(bucket_name, s3.get_paginator('list_objects_v2')))
            keys = [key for key in keys if ('unprocessed/' in key) and ('github-dedup' in key)]
            print(keys)
            temp_count = list()
            for key in tqdm.tqdm(keys):
                temp_count.append(key)
                if (key == keys[-1]) or (len(temp_count) == 100):
                    for z, zz in fetch_all(temp_count, s3):
                        pass
                    start = time.process_time()
                    json_datas = list(mypool.map(get_json, ['tmp/'+filename for filename in os.listdir('tmp')]))
                    print(f"Time taken dl: {time.process_time() - start}")
                    texts = list()
                    for i, json_data in enumerate(json_datas):
                        # start = time.process_time()
                        # Run preprocessing
                        ds = list(mypool.map(preprocess, json_data))

                        # Deduplicate hashes
                        uniques = set(item['hash'] for item in ds)
                        next_cache_append = copy.deepcopy(uniques)
                        frac = len(uniques) / len(ds)
                        # print(f"Fraction of duplicates: {1-frac:.2%}")

                        # print(f"Time taken process: {time.process_time() - start}")
                        # start = time.process_time()
                        # Deduplicate data and apply heuristics
                        t_start = time.time()
                        ds_filter = [dsfilter(ds_item, uniques=uniques, args=args, cached_uniques=cached_uniques) for ds_item in ds]
                        cached_uniques.update(next_cache_append)
                        # print(f"Time taken filter: {time.process_time() - start}")
                        # start = time.process_time()
                        pq.write_table(
                            pa.Table.from_pandas(pd.DataFrame([json_data[i] for i in range(len(json_data)) if ds_filter[i]])),
                            'outtmp/' + temp_count[i].replace('unprocessed/', '').replace('.json.gz', '.parquet'),
                            compression='NONE'
                        )
                        # print(f"Time taken jsonify: {time.process_time() - start}")
                    for z, zz in upload_all([temp.replace('unprocessed', 'hashdedup') for temp in temp_count], s3):
                        pass
                    shutil.rmtree('tmp')
                    shutil.rmtree('outtmp')
                    os.mkdir('tmp')
                    os.mkdir('outtmp')
        except botocore.exceptions.ClientError:
            print("finished.")
