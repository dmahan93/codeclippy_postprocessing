# Original file from https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/preprocessing.py
# butchered to directly support what I care about

import gzip
import hashlib
import json
import multiprocessing
import os
import re
import shutil
import time
from multiprocessing import Pool
from pathlib import Path
import boto3
import botocore.exceptions

import numpy as np
import tqdm
from datasets import load_dataset

from arguments import PreprocessingArguments
from transformers import AutoTokenizer, HfArgumentParser


PATTERN = re.compile(r"\s+")


def get_hash(example):
    """Get hash of content field."""
    return {"hash": hashlib.md5(re.sub(PATTERN, "", example["content"]).encode("utf-8")).hexdigest()}


def line_stats(example):
    """Calculates mean and max line length of file."""
    line_lengths = [len(line) for line in example["content"].splitlines()]
    return {"line_mean": np.mean(line_lengths), "line_max": max(line_lengths)}


def alpha_stats(example):
    """Calculates mean and max line length of file."""
    alpha_frac = np.mean([c.isalnum() for c in example["content"]])
    return {"alpha_frac": alpha_frac}


def check_uniques(example, uniques, cached_uniques):
    """Check if current hash is still in set of unique hashes and remove if true."""
    if example["hash"] in cached_uniques:
        return False
    elif example["hash"] in uniques:
        uniques.remove(example["hash"])
        return True
    else:
        return False


def is_autogenerated(example, scan_width=5):
    """Check if file is autogenerated by looking for keywords in the first few lines of the file."""
    keywords = ["auto-generated", "autogenerated", "automatically generated"]
    lines = example["content"].splitlines()
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return {"autogenerated": True}
    else:
        return {"autogenerated": False}


def is_config_or_test(example, scan_width=5, coeff=0.05):
    """Check if file is a configuration file or a unit test by :
    1- looking for keywords in the first few lines of the file.
    2- counting number of occurence of the words 'config' and 'test' with respect to number of lines.
    """

    keywords = ["unit tests", "test file", "configuration file"]
    lines = example["content"].splitlines()
    count_config = 0
    count_test = 0
    # first test
    for _, line in zip(range(scan_width), lines):
        for keyword in keywords:
            if keyword in line.lower():
                return {"config_or_test": True}
    # second test
    nlines = example["content"].count("\n")
    threshold = int(coeff * nlines)
    for line in lines:
        count_config += line.lower().count("config")
        count_test += line.lower().count("test")
        if count_config > threshold or count_test > threshold:
            return {"config_or_test": True}
    return {"config_or_test": False}


def has_no_keywords(example):
    """Check if a python file has none of the keywords for: funcion, class, for loop, while loop."""
    keywords = ["def ", "class ", "for ", "while "]
    lines = example["content"].splitlines()
    for line in lines:
        for keyword in keywords:
            if keyword in line.lower():
                return {"has_no_keywords": False}
    return {"has_no_keywords": True}


def has_few_assignments(example, minimum=4):
    """Check if file uses symbol '=' less than `minimum` times."""
    lines = example["content"].splitlines()
    counter = 0
    for line in lines:
        counter += line.lower().count("=")
        if counter > minimum:
            return {"has_few_assignments": False}
    return {"has_few_assignments": True}


def char_token_ratio(example):
    """Compute character/token ratio of the file with tokenizer."""
    input_ids = tokenizer(example["content"], truncation=False)["input_ids"]
    ratio = len(example["content"]) / len(input_ids)
    return {"ratio": ratio}


def preprocess(example):
    """Chain all preprocessing steps into one function to not fill cache."""
    results = dict()
    results.update(get_hash(example))
    results.update(line_stats(example))
    results.update(alpha_stats(example))
    results.update(char_token_ratio(example))
    results.update(is_autogenerated(example))
    results.update(is_config_or_test(example))
    results.update(has_no_keywords(example))
    results.update(has_few_assignments(example))
    return results


def dsfilter(example, uniques, args, cached_uniques)-> bool:
    """Filter dataset with heuristics. Config, test and has_no_keywords files are removed with a given probability."""
    if not check_uniques(example, uniques, cached_uniques):
        return False
    elif example["autogenerated"]:
        return False
    elif example["line_max"] > args.line_max:
        return False
    elif example["line_mean"] > args.line_mean:
        return False
    elif example["alpha_frac"] < args.alpha_frac:
        return False
    elif example["ratio"] < args.min_token_ratio:
        return False
    elif example["config_or_test"] and np.random.rand() <= args.filter_proba:
        return False
    elif example["has_no_keywords"] and np.random.rand() <= args.filter_proba:
        return False
    elif example["has_few_assignments"]:
        return False
    else:
        return True


def compress_file(file_path):
    """Compress a file with g-zip."""
    with open(file_path, "rb") as f_in:
        with gzip.open(str(file_path) + ".gz", "wb", compresslevel=6) as f_out:
            shutil.copyfileobj(f_in, f_out)
    os.unlink(file_path)

parser = HfArgumentParser(PreprocessingArguments)
args = parser.parse_args()
if args.num_workers is None:
    args.num_workers = multiprocessing.cpu_count()
tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)
if __name__ == '__main__':
    # Settings
    cached_uniques = set()

    s3 = boto3.client('s3')
    bucket_name = args.bucket_name
    with Pool(args.num_workers) as mypool:
        try:
            keys = [key['Key'] for key in s3.list_objects(Bucket=bucket_name)['Contents']]
            keys = [key for key in keys if ('unprocessed/' in key) and ('github-dedup' in key)]
            print(keys)
            for key in tqdm.tqdm(keys):
                s3.download_file(bucket_name, key, 'dl.json.gz')  #000000000000
                with gzip.open('dl.json.gz', 'rt', encoding='utf-8') as f:
                    data = f.read()
                    data = data.split('\n')
                    json_data = list()
                    count = 0
                    for line in data:
                        if line == '':
                            continue
                        count += 1
                        json_data.append(json.loads(line))

                # Run preprocessing
                ds = list(mypool.imap_unordered(preprocess, json_data))

                # Deduplicate hashes
                uniques = set(item['hash'] for item in ds)
                next_cache_append = set(item['hash'] for item in ds)
                frac = len(uniques) / len(ds)
                # print(f"Fraction of duplicates: {1-frac:.2%}")

                # Deduplicate data and apply heuristics
                t_start = time.time()
                ds_filter = [dsfilter(ds_item, uniques=uniques, args=args, cached_uniques=cached_uniques) for ds_item in ds]
                cached_uniques.update(next_cache_append)
                json_data = '\n'.join([json.dumps(json_data[i]) for i in range(len(json_data)) if ds_filter[i]])
                with gzip.open('out.json.gz', 'wt', compresslevel=6) as fout:
                    fout.write(json_data)
                resp = s3.upload_file('out.json.gz', bucket_name, key.replace("unprocessed", "hashdedup"))
        except botocore.exceptions.ClientError:
            print("finished.")
